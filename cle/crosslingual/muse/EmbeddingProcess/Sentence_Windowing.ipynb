{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from operator import itemgetter\n",
    "TAKEMOSTFREQUENTSENTENCES = 0.75\n",
    "from nltk import ngrams\n",
    "N = 5\n",
    "import itertools\n",
    "import sys\n",
    "regex = re.compile(\".*.{8}-.{4}-.{4}-.{4}-.{12}.*\")\n",
    "NGRAM_METHOD = True\n",
    "#!/usr/bin/python\n",
    "\n",
    "from multiprocessing import Process\n",
    "import multiprocessing\n",
    "import _thread\n",
    "import time\n",
    "from numpy import *\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "sentencedict = dict()\n",
    "inputfile = open(\"C:\\\\Users\\\\D072202\\\\RData2Graph\\\\rdata2graph\\\\data\\\\amazon_google_data\\\\corpus_1.txt\", \"r\")\n",
    "outputfile = open(\"C:\\\\Users\\\\D072202\\\\RData2Graph\\\\rdata2graph\\\\data\\\\amazon_google_data\\\\binary_corpus_1.txt\", \"w+\")\n",
    "for line in inputfile:\n",
    "    line = line.replace(\"\\n\",\"\")\n",
    "    if line in sentencedict:\n",
    "        sentencedict[line] = sentencedict[line] + 1\n",
    "    else:\n",
    "        sentencedict[line] = 1\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sents_sorted_by_occurence = list()\n",
    "#for key, value in sentencedict.items():\n",
    "#    temp = [key,value]\n",
    "#    sents_sorted_by_occurence.append(temp)\n",
    "#TAKEMOSTFREQUENTSENTENCES=99999\n",
    "#occurence_ctr=0\n",
    "#print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sents_sorted_by_occurence = sorted(sentencedict.items(), key=itemgetter(1))\n",
    "occurence_ctr = 0\n",
    "for sentence in sents_sorted_by_occurence:\n",
    "    occurence_ctr = occurence_ctr + sentence[1]\n",
    "TAKEMOSTFREQUENTSENTENCES = occurence_ctr*TAKEMOSTFREQUENTSENTENCES\n",
    "occurence_ctr = 0 \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word2ngrams(text, n=N, exact=True):\n",
    "    if (not NGRAM_METHOD):\n",
    "        return text;\n",
    "    else:\n",
    "        if (regex.match(text)):\n",
    "            text = text.split(\".\")\n",
    "            if (len(text)<2):\n",
    "                return [text[0]];\n",
    "            grams = [\"\".join(j) for j in zip(*[text[1][i:] for i in range(n)])]\n",
    "            return grams + [text[0]]\n",
    "        else:\n",
    "            return [\"\".join(j) for j in zip(*[text[i:] for i in range(n)])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence = \"<7a20467f-3aa2-450f-85d6-eb2e0a72760d> <HasChildren.#id> <421ccd3b-df77-4ce6-b53a-525c2b3e5134> <HasChildren.#id> <7a20467f-3aa2-450f-85d6-eb2e0a72760d> <Customer.lastname> <7a20467f-3aa2-450f-85d6-eb2e0a72760d.Mustermann>\"\n",
    "#ngrams = list()\n",
    "#for word in sentence.split():\n",
    "#    ngrams = ngrams + word2ngrams(word)\n",
    "#nparr = np.array(ngrams)\n",
    "#print(ngrams)\n",
    "#cartesian_product = np.transpose([np.tile(nparr, len(nparr)), np.repeat(nparr, len(nparr))])\n",
    "#print(cartesian_product)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(sents_sorted_by_occurencex, TAKEMOSTFREQUENTSENTENCESx, threadno, ngrammethod):\n",
    "    outputfilex = open(\"C:\\\\Users\\\\D072202\\\\RData2Graph\\\\rdata2graph\\\\data\\\\amazon_google_data\\\\trainingset\"+str(threadno)+\".txt\", \"w+\")\n",
    "    occurence_ctrx = 0\n",
    "    n=5\n",
    "    i=1\n",
    "    regexx = re.compile(\".*.{8}-.{4}-.{4}-.{4}-.{12}.*\")\n",
    "    blanklines=\"\"\n",
    "    while (i < threadno):\n",
    "        i=i+1\n",
    "        blanklines=blanklines+\"                         \"\n",
    "    for sentence in sents_sorted_by_occurencex:\n",
    "        if (occurence_ctrx > TAKEMOSTFREQUENTSENTENCESx):\n",
    "            break\n",
    "        ngrams = list()\n",
    "        words = set(sentence[0].split())\n",
    "        for word in words:\n",
    "            if (not ngrammethod):\n",
    "                ngrams = ngrams + [word]\n",
    "            else:\n",
    "                if (regexx.match(word)):\n",
    "                        word = word.split(\".\")\n",
    "                        if (len(word)<2):\n",
    "                            ngrams = ngrams + [word[0]];\n",
    "                        else:\n",
    "                            grams = [\"\".join(j) for j in zip(*[word[1][i:] for i in range(n)])]\n",
    "                            ngrams = ngrams + grams + [word[0]]\n",
    "                else:\n",
    "                        ngrams = ngrams + [\"\".join(j) for j in zip(*[word[i:] for i in range(n)])]\n",
    "        sys.stdout.write('%s\\r' % (blanklines + str(occurence_ctrx) + \"/\" + str(TAKEMOSTFREQUENTSENTENCESx)))\n",
    "        occurence_ctrx = occurence_ctrx + sentence[1]\n",
    "        nparr = np.array(ngrams)\n",
    "        cartesian_product = np.transpose([np.tile(nparr, len(nparr)), np.repeat(nparr, len(nparr))])\n",
    "        for pair in cartesian_product:\n",
    "            if pair[0] != pair[1]:\n",
    "                outputfilex.write(pair[0] + \",\" + pair[1]+\"\\n\")\n",
    "                outputfilex.write(pair[1] + \",\" + pair[0]+\"\\n\")\n",
    "    outputfilex.flush()\n",
    "    outputfilex.close()\n",
    "    sys.stdout.write('%s\\r' % (blanklines + str(TAKEMOSTFREQUENTSENTENCESx) + \"/\" + str(TAKEMOSTFREQUENTSENTENCESx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2408/3494.25\r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sys import platform\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "\n",
    "    sents1 = list()\n",
    "    sents2 = list()\n",
    "    sents3 = list()\n",
    "    sents4 = list()\n",
    "    ctr = 0;\n",
    "    octr = 0;\n",
    "    while (octr < TAKEMOSTFREQUENTSENTENCES/4):\n",
    "        sents1 = sents1 + [sents_sorted_by_occurence[ctr]]\n",
    "        ctr = ctr + 1\n",
    "        octr = octr + sents_sorted_by_occurence[ctr][1]\n",
    "    while (octr < TAKEMOSTFREQUENTSENTENCES/2):\n",
    "        sents2 = sents2 + [sents_sorted_by_occurence[ctr]]\n",
    "        ctr = ctr + 1\n",
    "        octr = octr + sents_sorted_by_occurence[ctr][1]\n",
    "    while (octr < TAKEMOSTFREQUENTSENTENCES*0.75):\n",
    "        sents3 = sents3 + [sents_sorted_by_occurence[ctr]]\n",
    "        ctr = ctr + 1\n",
    "        octr = octr + sents_sorted_by_occurence[ctr][1]\n",
    "    while (octr < TAKEMOSTFREQUENTSENTENCES):\n",
    "        sents4 = sents4 + [sents_sorted_by_occurence[ctr]]\n",
    "        ctr = ctr + 1\n",
    "        octr = octr + sents_sorted_by_occurence[ctr][1]\n",
    "    print(\"Running now.\")\n",
    "\n",
    "# Create two threads as follows\n",
    "\n",
    "    processes = [ ]\n",
    "    p1 = Process(target=generate, args=(sents1, TAKEMOSTFREQUENTSENTENCES/4, 1, NGRAM_METHOD,))\n",
    "    time.sleep(0.1)\n",
    "    p2 = Process(target=generate, args=(sents2, TAKEMOSTFREQUENTSENTENCES/4, 2, NGRAM_METHOD,))\n",
    "    time.sleep(0.1)\n",
    "    p3 = Process(target=generate, args=(sents3, TAKEMOSTFREQUENTSENTENCES/4, 3, NGRAM_METHOD,))\n",
    "    time.sleep(0.1)\n",
    "    p4 = Process(target=generate, args=(sents4, TAKEMOSTFREQUENTSENTENCES/4, 4, NGRAM_METHOD,))\n",
    "    processes.append(p1)\n",
    "    time.sleep(0.1)\n",
    "    processes.append(p2)\n",
    "    time.sleep(0.1)\n",
    "    processes.append(p3)\n",
    "    time.sleep(0.1)\n",
    "    processes.append(p4)\n",
    "    time.sleep(0.1)\n",
    "else:\n",
    "     generate(sents_sorted_by_occurence, TAKEMOSTFREQUENTSENTENCES, 0, NGRAM_METHOD,)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\nDone.\")\n",
    "\n",
    "while 1:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for line in open(\"C:\\\\Users\\\\D072202\\\\RData2Graph\\\\rdata2graph\\\\data\\\\amazon_google_data\\\\trainingset1.txt\",\"r\")\n",
    "#    outputfile.write(line)\n",
    "#for line in open(\"C:\\\\Users\\\\D072202\\\\RData2Graph\\\\rdata2graph\\\\data\\\\amazon_google_data\\\\trainingset2.txt\",\"r\")\n",
    "#    outputfile.write(line)\n",
    "#for line in open(\"C:\\\\Users\\\\D072202\\\\RData2Graph\\\\rdata2graph\\\\data\\\\amazon_google_data\\\\trainingset3.txt\",\"r\")\n",
    "#    outputfile.write(line)\n",
    "#for line in open(\"C:\\\\Users\\\\D072202\\\\RData2Graph\\\\rdata2graph\\\\data\\\\amazon_google_data\\\\trainingset4.txt\",\"r\")\n",
    "#    outputfile.write(line)\n",
    "#outputfile.flush()\n",
    "#outputfile.close()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "if (not True):\n",
    "                    ngrams = ngrams + word;\n",
    "                else:\n",
    "                    if (regex.match(word)):\n",
    "                        word = word.split(\".\")\n",
    "                        if (len(word)<2):\n",
    "                            return [word[0]];\n",
    "                        grams = [\"\".join(j) for j in zip(*[word[1][i:] for i in range(n)])]\n",
    "                        ngrams = ngrams + grams + [word[0]]\n",
    "                    else:\n",
    "                        ngrams = ngrams + [\"\".join(j) for j in zip(*[word[i:] for i in range(n)])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
